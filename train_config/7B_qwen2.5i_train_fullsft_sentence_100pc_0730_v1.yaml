### goal
# Train sentence Qwen2.5 7B Instruct on 2 H800 80GB
# 同样的设置，增加了batch size

### model
model_name_or_path: models/Qwen2.5-7B-Instruct-Braille
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
# pure_bf16: true
"bf16": true
# choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]
deepspeed: train_config/deepspeed/ds_z3_config.json  
# enable_liger_kernel: true
# use_unsloth_gc: true


### dataset
dataset: sentence_100pc_train_0727_v2
template: qwen # sharegpt
cutoff_len: 512 
max_samples: 500
overwrite_cache: true
preprocessing_num_workers: 4

### output
output_dir: saves/Qwen2.5-7B-Instruct-Braille/7B_sentence_100pc_qwen2.5i_train_fullsft_0727_v2/
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 250
gradient_accumulation_steps: 1
learning_rate: 4.0e-5
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.05
# bf16: true
ddp_timeout: 180000000
save_only_model: true
train_on_prompt: false
report_to: swanlab  # choices: [none, wandb, tensorboard, swanlab, mlflow]
flash_attn: auto
disable_shuffling: true

### eval
eval_dataset: sentence_100pc_val_0727_v2
per_device_eval_batch_size: 10
eval_strategy: steps
eval_steps: 500
# predict_with_generate: true
do_eval: true
# do_predict: true